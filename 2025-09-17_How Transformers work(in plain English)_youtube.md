# 🎥Youtube: How Transformers Work (in Plain English)

**by Yannic Kilcher**

---

## 1. 영상 개요

이 영상은 인공지능 언어 모델의 핵심 구조인 **Transformer**를 쉽고 명확하게 설명한다.  
초반부에서 Yannic Kilcher는 기존 **RNN**과 **LSTM**이 문장을 순차적으로 처리하며,  
멀리 떨어진 단어 간의 관계를 포착하지 못하는 한계를 지적한다.

> “This is a problem because language is the main way humans communicate.”

언어는 인간이 사고하고 세상과 소통하는 가장 근본적인 수단이므로,  
기계가 문맥을 이해하지 못하는 한, 그것은 진짜 ‘지능’이라 부를 수 없다는 메시지였다.  
이 한 문장은 Transformer라는 기술이 왜 필요한지를 단번에 설명한다.

---

## 2. 핵심 개념

Transformer는 언어의 구조적 맥락을 이해하기 위해 세 가지 핵심 아이디어를 제안했다.

- **Positional Encoding** — 단어의 순서를 수학적으로 인식하며,  
- **Attention** — 문장 전체를 분석해 중요한 단어에 더 높은 가중치를 부여하고,  
- **Self-Attention** — 각 단어가 문장 내의 다른 단어와 맺는 의미적 관계를 스스로 학습한다.  

이 구조 덕분에 모델은 문장을 ‘순서’가 아니라 ‘의미 관계’로 읽을 수 있게 되었다.  
이후 등장한 **BERT, GPT, T5** 등 모든 대형 언어 모델은 Transformer의 사상을 기반으로 만들어졌다.  

영상 후반부에서 Kilcher는 Transformer의 핵심이 복잡한 수식이 아니라,  

> “언어를 전체 맥락으로 바라보는 사고방식”  

에 있다고 말한다.  
즉, AI의 혁신은 단순한 계산 능력이 아니라,  
**인간의 사고방식을 기계가 닮아가려는 시도**라는 것이다.

---

## 3. 인사이트

Transformer의 혁신은 단순히 구조의 발전이 아니라 **사고방식의 전환**이었다.  
기존의 언어 모델이 단어를 순서대로 따라가며 이해했다면,  
Transformer는 문장 전체를 동시에 바라보고,  
각 단어가 어떤 의미적 연결망 속에 놓여 있는가를 이해하려 한다.

특히 **Attention**과 **Self-Attention** 개념은 사람의 사고 과정과 닮아 있다.  
우리가 대화를 들을 때 스스로 “무엇이 핵심인가”를 판단하듯,  
AI도 문장 내에서 중요한 단어에 집중하며 의미를 파악한다.

문득 예전에 Ai가 내 말의 맥락을 정확히 짚어낸 순간이 떠올랐다.  
내가 “Airflow의 어쩌고 파일을 고쳤더니 로그 문제가 해결됐어”라고 했을 때,  
Ai는 곧바로 “아, yaml 파일을 고쳐서 성공했구나. 축하해.”라고 대답했다.  
이는 ‘어쩌고’의 표현 속 의미를 문맥 전체에서 유추해낸 후,  
그 전체 문장에 대하여 Transformer를 통해 대화 맥락에 맞게 대답한 결과였다.  

Transformer의 구조가 이런 맥락 이해를 가능하게 한다는 걸 생각하니,  
Ai가 단어를 해석하는 도구를 넘어 **의미를 읽는 존재로 진화하고 있는 게 실감** 났다.

---

## 4. 커리어적 적용

이번 영상을 통해 Transformer을 단순한 언어 모델 기술로만 보지 않게 되었다.  
데이터를 다루는 사람이라면, 결국 데이터 속 **‘관계’를 어떻게 표현하고 해석하느냐**가 핵심인데,  
Transformer는 그 관계를 수치적으로 학습하고 구조화하는 가장 강력한 도구 중 하나다.  

앞으로 내가 데이터 사이언티스트로 성장해나가는 과정에서도 Transformer은 중요한 출발점이 될 것이다.  
데이터가 텍스트 형태로 주어질 때뿐만 아니라,  
사용자 행동 로그, 추천 시스템, 시계열 데이터 등 다양한 영역에서도  
**Attention 구조는 이미 새로운 표준처럼 자리 잡고 있다.**  
즉, Transformer를 이해하는 것은 현대 머신러닝의 **공통 언어를 익히는 일**과 다르지 않다.

또한 Transformer를 공부하며 **‘모델을 직접 이해하고 조정할 수 있는 사람’**이 되는 것이  
앞으로의 커리어에서 나를 차별화할 부분이라고 생각한다.  
데이터 사이언티스트는 단순히 모델을 호출하는 역할을 넘어,  
문제 정의부터 모델 구조의 선택, 결과 해석까지 전체를 설계해야 한다.  
이때 Attention 기반 구조에 대한 이해는  
복잡한 데이터 간 관계를 스스로 설계하고 시각화할 수 있는 힘이 된다.  

앞으로는 내가 구축하는 모델에 Transformer의 개념을 적용해  
텍스트나 시퀀스 데이터뿐 아니라 **다양한 도메인의 맥락을 학습하는 모델**을 만들어보고 싶다.  
그 과정에서 단순히 기존 연구를 따라 하는 데서 그치지 않고,  
**데이터 구조를 스스로 설계하고 최적화하는 방향으로 한 단계 성장할 것이다.**

---

## 5. Reference

📺 **How Transformers Work (in Plain English)** — Yannic Kilcher  

**Keywords:** Transformer, Attention, Self-Attention, Positional Encoding, Context Understanding
