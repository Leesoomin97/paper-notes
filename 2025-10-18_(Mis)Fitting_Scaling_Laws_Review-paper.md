# (Mis) Fitting: A Survey of Scaling Laws

> **논문 리뷰 / Scaling Laws / ICLR 2025**  
> https://arxiv.org/abs/2502.18969  
>  
> **저자:** Margaret Li, Sceha Kudugunta, Luke Zettlemoyer  
> **분야:** Machine Learning / Meta-Science / Scaling Laws  

---

## 1. 논문 개요

AI 활용 설정  
사진 설명을 입력하세요.  

**제목:** (Mis) Fitting: A Survey of Scaling Laws  
**저자:** Margaret Li, Sceha Kudugunta, Luke Zettlemoyer  
**학회:** ICLR 2025  
**분야:** Machine Learning / Meta-Science / Scaling Laws  
**사이트:** [https://arxiv.org/abs/2502.18969](https://arxiv.org/abs/2502.18969)

---

## 2. 연구 내용

### (1) 연구 배경

최근 GPT, Llama, Claude 같은 대형 언어 모델(LLM)은 모델 크기, 데이터양, 연산량 등과 성능 사이의 관계를 나타내는 스케일링 법칙에 의해 설계되는 경우가 많다.  

하지만 Li et al.(2025)의 "(Mis) Fitting: A Survey of Scaling Laws"는 이 법칙이 실제로는 실험 세부 설정에 따라 크게 달라지며, 논문마다 그 조건이 충분히 공개되지 않아 재현성과 객관성이 위협받고 있다고 지적한다.

---

### (2) 주요 내용 정리

#### 1) 스케일링 법칙의 불일치

저자들은 50편 이상의 Scailing Law 관련 논문을 조사했으나, 이중 45편이 Power Law 형태를 사용했지만, 다음과 같은 핵심 정보가 자주 누락되어 있었다.

- 사용된 데이터 셋의 세부 정보  
- fitting 범위 및 기준  
- 손실 함수의 정의  
- 학습 세팅(optimizer, batch size 등)  

결과적으로 각 논문에서 도출된 기울기(scaling exponent)가 다르고 결론도 일관되지 않았다.

AI 활용 설정  
사진 설명을 입력하세요.  

예를 들어,  
- Kaplan et al.(OpenAI, 2020)은 **"토큰: 파라미터 = 20 : 1"**  
- Hoffmann et al.(DeepMind, 2022)은 **"5:1"**  
- Chinchilla 연구는 **"1:1"**  
이라는 서로 다른 결론을 제시한다.

이 차이는 단순한 오차가 아니라, 세부 설정의 차이로 인한 구조적 불일치라는 것이다.

---

#### 2) 실험 세부의 영향

Li et al. 은 스케일링 결과에 영향을 미치는 핵심 요인을 세 가지로 정리했다.  

① **데이터 처리 방식** - 중복 제거, 토큰화, 샘플링 정책  
② **모델 구조** - 레이어 깊이, width/depth 비율, normalization 방식  
③ **훈련 설정** - Learning rate, optimizer, batch size  

이 중 하나라도 달라지면 스케일링 곡선의 기울기와 intercept가 바뀌며, 최적 토큰 수 같은 결론조차 달라질 수 있다.  

즉, 스케일링 법칙은 실험 환경에 매우 민감한 경험적 관계이지, 보편적인 물리 법칙이 아니다.

---

#### 3) 재현성 위기

저자들은 또 다른 심각한 문제로 **재현 불가능성**을 지적한다.  

대부분의 논문이 코드, 로그, 모델 가중치, fitting 과정 등을 공개하지 않았기 때문에 다른 연구자가 같은 결과를 얻을 수 없다.  

즉, 스케일링 법칙은 보편적 과학 법칙이 아니라 **특정 환경에서의 경험적 추세**에 불과한 셈이다.

---

### (3) 핵심 인사이트 요약

이 논문이 제시한 핵심 인사이트는 세 가지로 압축된다.

1️⃣ 같은 데이터라도 어떤 fitting 함수를 쓰느냐에 따라 결과가 완전히 달라진다.  
2️⃣ 여러 연구가 제시한 최적 토큰 - 파라미터 비율이 서로 다르다.  
3️⃣ 스케일링 연구의 신뢰성을 높이기 위해 데이터, 모델 구조, 손실 함수, fitting 범위, 평가 방식, 코드 공개 여부를 명시해야 한다는 **체크리스트(Scaling Law Checklist)**를 제안했다.

---

### (4) 스케일링 연구 체크 리스트

| 항목 | 공개 필요 내용 | 예시 |
|------|----------------|------|
| 데이터 | 크기, 중복 제거 방식 | 400B 토큰, dedup 적용 |
| 모델 구조 | 레이어 수, width/depth 비율 | 70B params, depth 80 |
| 손실 함수 | 사용 기준 명시 | cross-entropy |
| fitting 범위 | 적용 구간 | 10^6 ~ 10^9 tokens |
| 코드 공개 | 로그, 재현 스크립트 | GitHub link |

앞으로 스케일링 연구는 성능곡선만 보여주는 그래프가 아니라 **재현 가능한 실험 기록과 함께 공개**되어야 한다.

---

## 3. 나의 생각

이 논문을 읽으며 느낀 건, 스케일링 법칙의 불안정성보다 더 근본적인 문제는 **AI 연구 환경의 불균형**이라는 점이었다.

논문마다 다른 결과가 나오는 이유는 단순한 실험 설정의 차이 만이 아니라,  
① 산업 중심의 연구 구조, ② 자원 접근성의 불균등, ③ 결과 위주의 논문 문화 때문이라고 생각한다.  
기업은 경쟁력을 위해 데이터와 코드를 공개하지 않고, 대부분의 연구자는 한정된 자원으로 인해, Colab이나 Kaggle, 또는 개인의 로컬 컴퓨터와 같은 제한된 환경에서 실험한다.

결과적으로 성능이 아니라 자원이 결과를 결정하는 구조가 되는 것이다.

나 역시 Colab이 끊기거나 메모리가 부족해 모델 실험 시 epoch를 다 못 돌린 적이 많았다.  
그때마다 느낀 건 내 능력이 부족한 것도 맞지만 **환경 역시 다르다**는 사실이었다.  
따라서 논문의 저자가 주장하는 내용에 대하여 연구자에게 필요한 **정직한 기준선**이라고 동의하며 긍정한다.

그래서 나는 이렇게 제안하고 싶다.  
논문은 실험 환경(GPU, 메모리, 시간)을 명시하고, Colab이나 1GPU로 재현 가능한 축소 버전 실험을 함께 공개하면 좋겠다.  
또한 공공 데이터셋이 더 많이 개방되어야 한다.  
그래야만 진짜 비교가 가능하고, 저연차 연구자들도 같은 출발선에서 시작할 수 있다.

---

**출처:**  
Li, M., Kudugunta, S., & Zettlemoyer, L. (2025). *(Mis) Fitting: A Survey of Scaling Laws.* ICLR 2025.  
https://arxiv.org/abs/2502.18969
