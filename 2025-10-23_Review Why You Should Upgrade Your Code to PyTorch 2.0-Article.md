# Review: Why You Should Upgrade Your Code to PyTorch 2.0

## 1. 기사 요약: “이제는 업그레이드해야 할 때”

Weights & Biases의 **Thomas Capelle**은 최근 글 *〈Why You Should Upgrade Your Code to PyTorch 2.0〉*에서  
PyTorch 2.0이 단순한 버전 업그레이드가 아닌 **엔진 교체급 혁신**임을 강조했다.  
그는 “API 변경 없이도 코드 한 줄만으로 GPU 처리량을 30~50% 높일 수 있다”고 밝혔다.

핵심은 두 가지 기능이다.

1. `torch.set_default_device()` — 한 번의 설정으로 전체 코드의 디바이스를 지정  
2. `torch.compile()` — 기존 모델을 컴파일 최적화하여 GPU 커널 효율을 대폭 향상  

Capelle은 **ResNet50**과 **BERT** 모델을 A100, V100 GPU에서 벤치마크한 결과를 제시했다.  

- ResNet50: A100에서 **+33%**, V100에서 **+25%**  
- BERT: A100에서 **+45%**, V100에서 **+50%**  

또한 **DataLoader 처리 속도**와 **CPU 활용도**가 Docker 환경에서 더 안정적이었으며,  
Conda 환경에서는 스레드 활용이 제한되어 병목이 생겼다고 언급했다.  
그는 결론적으로, “Docker 기반의 PyTorch 2.0은 더 이상 실험적 기능이 아닌 실전용 도구”라고 평가했다.

---

## 2. 기술 분석: PyTorch 2.0은 어떻게 빨라졌는가  

### (1) Eager Mode의 한계와 컴파일러의 등장

기존 **PyTorch 1.x**는 **즉시 실행(Eager Execution)** 방식을 사용했다.  
모델의 연산이 Python 인터프리터를 거쳐 **한 줄씩 실행**되므로 디버깅은 쉬웠지만,  
GPU가 매번 짧은 커널 호출을 반복해야 해 병목이 발생했다.

반면 **PyTorch 2.0의 `torch.compile()`**은 이 과정을 완전히 바꾼다.  
2.0의 실행 시 내부적으로는 다음 단계가 일어난다.

1. **`torch._dynamo`** — Python 바이트코드를 분석해 연산 그래프를 추출  
2. **`AOTAutograd`** — 미분 연산을 미리 그래프로 생성해 모든 epoch마다 그래프를 새로 만들 필요 제거  
3. **`torch._inductor`** — 여러 연산을 하나의 커널로 병합(fusion)해 메모리 이동과 호출 횟수 최소화  

즉, 연산을 미리 최적화해 **한 번에 큰 묶음으로 처리**하는 것이다.  
GPU는 더 이상 수백 번의 호출을 기다리지 않아도 된다.  

비유하자면,  
> PyTorch 1.x는 수동 기어였다면,  
> PyTorch 2.0은 자동 변속기와 같다 — 속도는 유지하면서 효율이 극대화된다.

---

### (2) 이전 버전과의 비교

| 구분 | PyTorch 1.x | PyTorch 2.0 (`torch.compile`) |
|------|--------------|------------------------------|
| 실행 방식 | 즉시 실행(동적 그래프) | 사전 컴파일(정적 그래프) |
| Python 개입 | 연산마다 개입 | 최초 1회 분석 후 자동 실행 |
| 커널 호출 | 수백~수천 회 | 병합 후 수십 회 |
| 속도 | 상대적으로 느림 | 최대 2배 향상 가능 |
| 디버깅 | 자유로움 | 다소 제약 있음 (스택 제한) |

결과적으로 PyTorch 2.0은 Python의 느림을 한 번만 감수하고,  
이후에는 GPU가 전속력으로 달리는 구조이다.  
첫 실행 시 약간의 컴파일 시간이 추가되지만,  
**훈련이 길어질수록 속도 이득은 압도적으로 커진다.**

---

### (3) 유의사항과 Trade-off

- `torch.compile()`을 사용하면 그래프가 정적으로 변하기 때문에  
  기존 버전과는 달리 **디버깅 시 내부 연산을 직접 추적하기 어렵다.**  
  필요할 때 `fullgraph=False` 옵션을 주면 부분 컴파일로 전환 가능하다.  

- **정적 그래프는 입력 형태나 dtype이 바뀌면 자동으로 무효화**되어  
  다시 컴파일을 수행하므로 오차 누적은 발생하지 않는다.  
  즉, “고정된 그래프를 재사용”하는 것이 아니라 **입력이 같을 때만 재사용**한다.  

- Docker 환경에서는 CPU 스레드 활용률이 높아 DataLoader 병목이 거의 없지만,  
  Conda 환경에서는 여전히 병렬 처리 효율이 낮을 수 있다.  

결론적으로,  
> PyTorch 2.0은 기존 버전의 손쉬운 디버깅 대신 **빠른 속도**를 택했다.  
> 연구 단계에서는 **Eager Mode**,  
> 최종 훈련 단계에서는 **Compile Mode**를 적용하는 전략이 이상적이다.

---

## 3. PyTorch 2.0이 가져올 미래의 변화  

PyTorch 2.0은 단순한 속도 개선이 아니라 **AI 컴퓨팅 패러다임의 진화**다.  
이전까지 프레임워크가 GPU 계산을 “프로그래머 친화적”으로 다루었다면,  
이제는 “컴파일러 친화적”으로 변화하고 있다.  

이 변화가 가져올 파급력은 세 가지다.

### (1) 연구 효율성의 비약적 향상
코드를 수정하지 않아도 30~50%의 훈련 속도 향상을 얻을 수 있으므로  
연구자의 **실험 주기(iteration cycle)** 가 크게 단축된다.  
하이퍼파라미터 탐색, 모델 구조 실험이 훨씬 가벼워진다.

### (2) 대규모 모델 훈련의 경제성
Transformer와 Diffusion 같은 초대형 모델의 GPU 비용이 줄어든다.  
`torch.compile()`은 커널 병합 덕분에 **전력 소모와 VRAM 낭비를 최소화**한다.  
이는 “모델의 크기보다 효율이 더 중요한 시대”로의 전환을 의미한다.

### (3) AI 생태계의 자동화 기반 확립
PyTorch 2.0은 AI 프레임워크를 **자동 최적화 시스템**으로 진화시켰다.  
앞으로의 고도화된 모델(GPT, Diffusion, RLHF 등)은  
이런 “컴파일러 레벨 최적화” 위에서 학습될 것이다.

---

## 4. 마무리하며  

PyTorch 2.0의 등장은 단순한 속도 개선이 아니라  
**AI 연구 전반이 컴파일러 중심의 자동 최적화 단계로 진입했음을 알리는 신호**다.  

이제 모델의 성능은 단순히 파라미터 수가 아니라,  
**연산 그래프를 얼마나 효율적으로 다루는가**,  
그리고 **하드웨어·소프트웨어의 조화**에 달려 있다.  

대형 모델들이 점점 복잡해지는 지금,  
PyTorch 2.0과 같은 기술적 발전은 **고도화된 AI로 향하는 발걸음** 그 자체다.  
앞으로의 PyTorch 신규 버전은 단순한 업데이트가 아니라,  
**AI의 새로운 형태를 여는 컴파일러 세대의 진화**가 될 것이다.

---

## 📚 참고 자료  
- Thomas Capelle, *Why You Should Upgrade Your Code to PyTorch 2.0*, Weights & Biases Reports (2023.03.22)  
  [W&B 원문 보기](https://wandb.ai/capecape/pt2/reports/Why-You-Should-Upgrade-Your-Code-to-PyTorch-2-0--VmlldzozODUyMzcw)

- PyTorch Team, *Introducing PyTorch 2.0: Faster, More Pythonic, and as Dynamic as Ever*  
  [PyTorch 공식 블로그](https://pytorch.org/blog/pytorch-2.0/)

---

